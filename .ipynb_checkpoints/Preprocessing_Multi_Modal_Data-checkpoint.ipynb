{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "survey = pd.read_csv('hmm/survey.csv')\n",
    "calls = pd.read_csv('hmm/calls.csv')\n",
    "locations = pd.read_csv('hmm/Semantic_locations.csv')\n",
    "sias =  pd.read_csv('hmm/SIAS.csv')\n",
    "sms = pd.read_csv('hmm/sms.csv')\n",
    "acc = pd.read_csv('hmm/minutes_level_acc_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates new column for date by stripping off the time zone\n",
    "acc['minutes.2']= acc['minutes.1'].astype(str).str[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertToLong(x):\n",
    "    \"\"\"\n",
    "    Takes a columns of date formated as '%Y-%m-%d %H:%M:%S' \n",
    "    Returns: long format/ format in microseconds\n",
    "    \"\"\"\n",
    "    return int(time.mktime(datetime.datetime.strptime(x,  '%Y-%m-%d %H:%M:%S').timetuple())*1000)\n",
    "\n",
    "#Apply the above function to create column\n",
    "acc['date_long'] =  acc.apply(lambda x: convertToLong(x['minutes.2']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating new data frame with last location in the last 2 hours for each survey time\n",
    "\n",
    "store1 = []\n",
    "\n",
    "for i  in survey['pid'].unique():\n",
    "    for j in survey[survey[\"pid\"] == i]['date_long'].values:\n",
    "        #print i, j\n",
    "        d = locations[locations['pid']== i] \n",
    "        #d = des.sort_values(['date'],ascending=True).groupby('pid')\n",
    "        #looking at interval of 20 mins before and after time to infer current location\n",
    "        #This could be reduced to get less sample\n",
    "        ind  = d[(d['startdatepoi'] >= j - 7200000) & (d['startdatepoi'] < j) ].index.tolist()\n",
    "        #print i, ind\n",
    "        if ind==[]:\n",
    "             store1.append({'pid': i, 'date': j, 'lastlocationin2': 'NaN' })\n",
    "        else:\n",
    "            #print locations.ix[ind[0], 'pid'], locations.ix[ind[0], 'date'], locations.loc[ind[0], 'duration']\n",
    "            store1.append({'pid': i, 'date': j, 'lastlocationin2':  locations.loc[max(ind), 'location']})\n",
    "\n",
    "                   \n",
    "newdat1 = pd.DataFrame(store1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating new dataframe with total number of texts messages both received and sent\n",
    "\n",
    "store2 = []\n",
    "\n",
    "for i  in survey['pid'].unique():\n",
    "    for j in survey[survey[\"pid\"] == i]['date_long'].values:\n",
    "        #print i, j\n",
    "        d = sms[sms['pid']== i] \n",
    "        #d = des.sort_values(['date'],ascending=True).groupby('pid')\n",
    "        #looking at interval of 20 mins before and after time to infer current location\n",
    "        #This could be reduced to get less sample\n",
    "        ind  = d[(d['date'] >= j - 3600000) & (d['date'] < j) ].index.tolist()\n",
    "        #print i, len(ind)\n",
    "        store2.append({'pid': i, 'date': j, '#ofTextIn1Hour': len(ind)})\n",
    "\n",
    "\n",
    "newdat2 = pd.DataFrame(store2)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data frame to create total number of calls \n",
    "store3 = []\n",
    "\n",
    "for i  in survey['pid'].unique():\n",
    "    for j in survey[survey[\"pid\"] == i]['date_long'].values:\n",
    "        #print i, j\n",
    "        d = calls[calls['pid']== i] \n",
    "        #d = des.sort_values(['date'],ascending=True).groupby('pid')\n",
    "        #looking at interval of 20 mins before and after time to infer current location\n",
    "        #This could be reduced to get less sample\n",
    "        ind  = d[(d['date_end'] >= j - 3600000) & (d['date_end'] < j) ].index.tolist()\n",
    "        #print i, len(ind)\n",
    "        store3.append({'pid': i, 'call_end': j, '#ofCallsIn1Hour': len(ind)})\n",
    "\n",
    "\n",
    "newdat3 = pd.DataFrame(store3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating CallDuration column for the calls dataframe\n",
    "calls['callDuration'] = (calls['date_end'] - calls['date_start']) / 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating Mean and Max duration of calls in the last our\n",
    "store4 = []\n",
    "\n",
    "for i  in survey['pid'].unique():\n",
    "    for j in survey[survey[\"pid\"] == i]['date_long'].values:\n",
    "        #print i, j\n",
    "        d = calls[calls['pid']== i] \n",
    "        #Using end-date as reference time point because it marks the end of one episode\n",
    "        #and we can capture more episode looking back in 1 hour window\n",
    "        #This could be reduced to get less sample\n",
    "        ind  = d[(d['date_end'] >= j - 3600000) & (d['date_end'] < j) ].index.tolist()\n",
    "        #print i, ind\n",
    "        if ind==[]:\n",
    "            store4.append({'pid': i, 'date': j, 'MeanDuratin1Hour': 0, 'MaxDuratin1Hour': 0 })\n",
    "        else:\n",
    "            #print locations.ix[ind[0], 'pid'], locations.ix[ind[0], 'date'], locations.loc[ind[0], 'duration']\n",
    "            store4.append({'pid': i, 'date': j, 'MeanDuratin1Hour':  np.mean(calls.loc[ind, 'callDuration']),\n",
    "                          'MaxDuratin1Hour':  np.max(calls.loc[ind, 'callDuration'])})\n",
    "\n",
    "            \n",
    "newdat4 = pd.DataFrame(store4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data frame to create total number of calls (Outgoing)\n",
    "store31 = []\n",
    "\n",
    "for i  in survey['pid'].unique():\n",
    "    for j in survey[survey[\"pid\"] == i]['date_long'].values:\n",
    "        #print i, j\n",
    "        d = calls[(calls['pid']== i) & (calls['state']== 1)]  \n",
    "        #d = des.sort_values(['date'],ascending=True).groupby('pid')\n",
    "        #looking at interval of 20 mins before and after time to infer current location\n",
    "        #This could be reduced to get less sample\n",
    "        ind  = d[(d['date_end'] >= j - 3600000) & (d['date_end'] < j) ].index.tolist()\n",
    "        #print i, len(ind)\n",
    "        store31.append({'pid': i, 'call_end': j, '#ofCallsIn1HourOut': len(ind)})\n",
    "\n",
    "\n",
    "newdat31 = pd.DataFrame(store31)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating all the variables we extract from accelerometer data\n",
    "store5 = []\n",
    "\n",
    "for i  in survey['pid'].unique():\n",
    "    for j in survey[survey[\"pid\"] == i]['date_long'].values:\n",
    "        #print i, j\n",
    "        d = acc[acc['pid']== i] \n",
    "        #Using end-date as reference time point because it marks the end of one episode\n",
    "        #and we can capture more episode looking back in 1 hour window\n",
    "        #This could be reduced to get less sample\n",
    "        ind  = d[(d['date_long'] >= j - 3600000) & (d['date_long'] < j) ].index.tolist()\n",
    "        #print i, ind\n",
    "        if ind==[]:\n",
    "            store5.append({'pid': i, 'date': j, 'avgAccMean': 0, 'avgAccSD': 0,  'minAccMean': 0, 'minAccSD': 0,\n",
    "                           'maxAccMean': 0, 'maxAccSD': 0, 'stdAccMean': 0, 'stdAccSD': 0, 'medianAccMean': 0 , 'medianAccSD': 0\n",
    "                          ,'varAccMean': 0, 'varAccSD': 0})\n",
    "        else:\n",
    "            #print locations.ix[ind[0], 'pid'], locations.ix[ind[0], 'date'], locations.loc[ind[0], 'duration']\n",
    "            store5.append({'pid': i, 'date': j, 'avgAccMean':  np.mean(acc.loc[ind, 'avg']),  'avgAccSD':  np.std(acc.loc[ind, 'avg']),\n",
    "                           'minAccMean': np.mean(acc.loc[ind, 'min']), 'minAccSD': np.std(acc.loc[ind, 'min']), \n",
    "                           'maxAccMean': np.mean(acc.loc[ind, 'max']), 'maxAccSD': np.std(acc.loc[ind, 'max']),\n",
    "                           'stdAccMean': np.mean(acc.loc[ind, 'stddev']), 'stdAccSD': np.std(acc.loc[ind, 'stddev']), \n",
    "                           'medianAccMean': np.mean(acc.loc[ind, 'median']), 'medianAccSD': np.std(acc.loc[ind, 'median']),\n",
    "                          'varAccMean': np.mean(acc.loc[ind, 'variance']), 'varAccSD': np.std(acc.loc[ind, 'variance'])})\n",
    "\n",
    "        \n",
    "\n",
    "newdat5 = pd.DataFrame(store5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating all the variables we extract from accelerometer data\n",
    "storing = []\n",
    "\n",
    "for i  in survey['pid'].unique():\n",
    "    for j in survey[survey[\"pid\"] == i]['date_long'].values:\n",
    "        #print i, j\n",
    "        d = acc[acc['pid']== i] \n",
    "        #Using end-date as reference time point because it marks the end of one episode\n",
    "        #and we can capture more episode looking back in 1 hour window\n",
    "        #This could be reduced to get less sample\n",
    "        ind  = d[(d['date_long'] >= j - 1800000) & (d['date_long'] < j) ].index.tolist()\n",
    "        #print i, ind\n",
    "        if ind==[]:\n",
    "            storing.append({'pid': i, 'date': j, 'avgAccMean30': 0, 'avgAccSD30': 0,  'minAccMean30': 0, 'minAccSD30': 0,\n",
    "                           'maxAccMean30': 0, 'maxAccSD30': 0, 'stdAccMean30': 0, 'stdAccSD30': 0, 'medianAccMean30': 0 , 'medianAccSD30': 0\n",
    "                          ,'varAccMean30': 0, 'varAccSD30': 0})\n",
    "        else:\n",
    "            #print locations.ix[ind[0], 'pid'], locations.ix[ind[0], 'date'], locations.loc[ind[0], 'duration']\n",
    "            storing.append({'pid': i, 'date': j, 'avgAccMean30':  np.mean(acc.loc[ind, 'avg']),  'avgAccSD30':  np.std(acc.loc[ind, 'avg']),\n",
    "                           'minAccMean30': np.mean(acc.loc[ind, 'min']), 'minAccSD30': np.std(acc.loc[ind, 'min']), \n",
    "                           'maxAccMean30': np.mean(acc.loc[ind, 'max']), 'maxAccSD30': np.std(acc.loc[ind, 'max']),\n",
    "                           'stdAccMean30': np.mean(acc.loc[ind, 'stddev']), 'stdAccSD30': np.std(acc.loc[ind, 'stddev']), \n",
    "                           'medianAccMean30': np.mean(acc.loc[ind, 'median']), 'medianAccSD30': np.std(acc.loc[ind, 'median']),\n",
    "                          'varAccMean30': np.mean(acc.loc[ind, 'variance']), 'varAccSD30': np.std(acc.loc[ind, 'variance'])})\n",
    "\n",
    "        \n",
    "\n",
    "newdating = pd.DataFrame(storing) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
